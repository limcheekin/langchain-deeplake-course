{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Intro to Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_BASE = os.environ[\"COMPLETIONS_API_BASE\"]\n",
    "SYSTEM_PROMPT_TEMPLATE = \"You are an AI assistant that follows instruction extremely well. Help as much as you can.\"\n",
    "# orca-mini-v3\n",
    "def get_prompt_template(instruction: str):\n",
    "    prompt = f\"### System:\\n{SYSTEM_PROMPT_TEMPLATE}\\n\\n### User:\\n{instruction}\\n\\n### Assistant:\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man walks into a bar with his pet monkey. The bartender looks up and says, \"Hey, I've never seen anything like this in my whole life.\" The man explains, \"Yeah, he's a great drinker.\" The bartender raises an eyebrow and asks, \"Really? What does he drink?\" The man answers, \"Long Island Iced Tea.\"\n",
      "Tokens Used: 137\n",
      "\tPrompt Tokens: 46\n",
      "\tCompletion Tokens: 91\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0027400000000000002\n",
      "CPU times: user 156 ms, sys: 4.83 ms, total: 161 ms\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(openai_api_base=OPENAI_API_BASE, n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(get_prompt_template(\"Tell me a joke.\"))\n",
    "    print(result)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the OpenAI API, the `n` parameter specifies the number of completions to generate for a given prompt, while the `best_of` parameter controls the number of candidate completions generated server-side⁶. The API then returns the `n` \"best\" completions (the ones with the highest log probability per token) out of the `best_of` candidates⁶.\n",
    "\n",
    "In the code you provided, both `n` and `best_of` are set to 2, which means that the API will generate 2 candidate completions and return the 2 \"best\" ones. Is there anything else you would like to know? 😊\n",
    "\n",
    "Source: Conversation with Bing, 8/17/2023\n",
    "(1) undefined. https://platform.openai.com/docs/api-reference/introduction.\n",
    "(2) OpenAI Platform. https://platform.openai.com/docs/api-reference/introduction..\n",
    "(3) How is the search among 'best_of' for 'n' completions conducted in the .... https://stackoverflow.com/questions/76009968/how-is-the-search-among-best-of-for-n-completions-conducted-in-the-openai-co.\n",
    "(4) What is the OpenAI API warning: To avoid an invalid_request_error, best .... https://stackoverflow.com/questions/75196860/what-is-the-openai-api-warning-to-avoid-an-invalid-request-error-best-of-was-s.\n",
    "(5) undefined. https://api.openai.com/v1/models.\n",
    "(6) undefined. https://api.openai.com/v1/chat/completions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! stop is not default parameter.\n",
      "                    stop was transferred to model_kwargs.\n",
      "                    Please confirm that stop is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following are excerpts from conversations with an AI\n",
      "assistant. The assistant is known for its humor and wit, providing\n",
      "entertaining and amusing responses to users' questions. Here are some\n",
      "examples:\n",
      "\n",
      "\n",
      "\n",
      "User: What's the weather like?\n",
      "AI: It's raining cats and dogs, better bring an umbrella!\n",
      "\n",
      "\n",
      "\n",
      "User: How old are you?\n",
      "AI: Age is just a number, but I'm timeless.\n",
      "\n",
      "\n",
      "\n",
      "User: What's the meaning of life?\n",
      "AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The meaning of life is to find out what makes you happy and do that as often as possible.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(openai_api_base=OPENAI_API_BASE,  \n",
    "                  temperature=0.0,\n",
    "                  stop=[\"\\n\\n\"], \n",
    "                  max_tokens=128)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template, verbose=True)\n",
    "chain.run(\"What's the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples with Easy Prompts: Text Summarization, Text Translation, and Question Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Question-Answering Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=get_prompt_template(template),\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### System:\n",
      "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
      "\n",
      "### User:\n",
      "Question: What is the capital city of France?\n",
      "\n",
      "Answer: \n",
      "\n",
      "### Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " The capital city of France is Paris.\n",
      "CPU times: user 9.94 ms, sys: 996 µs, total: 10.9 ms\n",
      "Wall time: 8.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=chat,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text=' The capital city of France is Paris.', generation_info=None, message=AIMessage(content=' The capital city of France is Paris.', additional_kwargs={}, example=False))], [ChatGeneration(text=' The largest living terrestrial mammal is the African bush elephant (Loxodonta africana).', generation_info=None, message=AIMessage(content=' The largest living terrestrial mammal is the African bush elephant (Loxodonta africana).', additional_kwargs={}, example=False))], [ChatGeneration(text=\" The most abundant gas in Earth's atmosphere is nitrogen (N2), making up approximately 78.09% of the total mass. Oxygen (O2) makes up the second-most abundant gas, accounting for about 20.95%.\", generation_info=None, message=AIMessage(content=\" The most abundant gas in Earth's atmosphere is nitrogen (N2), making up approximately 78.09% of the total mass. Oxygen (O2) makes up the second-most abundant gas, accounting for about 20.95%.\", additional_kwargs={}, example=False))], [ChatGeneration(text=' A ripe banana is usually yellow in color.', generation_info=None, message=AIMessage(content=' A ripe banana is usually yellow in color.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 259, 'completion_tokens': 104, 'total_tokens': 363}, 'model_name': 'gpt-3.5-turbo'} run=RunInfo(run_id=UUID('dd8bb143-6258-483d-8db2-afe6ff3f7a9f'))\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### System:\n",
      "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
      "\n",
      "### User:\n",
      "Answer the following questions one at a time.\n",
      "\n",
      "Questions:\n",
      "What is the capital city of France?\n",
      "What is the largest mammal on Earth?\n",
      "Which gas is most abundant in Earth's atmosphere?\n",
      "What color is a ripe banana?\n",
      "\n",
      "\n",
      "Answers:\n",
      "\n",
      "\n",
      "### Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 11 ms, sys: 313 µs, total: 11.3 ms\n",
      "Wall time: 19.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The capital city of France is Paris.\\nThe largest mammal on Earth is the blue whale.\\nThe most abundant gas in Earth's atmosphere is nitrogen (N2).\\nA ripe banana is yellow.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=get_prompt_template(multi_template), \n",
    "            input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], \n",
    "                            template=get_prompt_template(summarization_template))\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangChain offers modules for language model applications and can be combined or used individually for various use cases, with a basic example demonstrating generating a company name based on its product.\n"
     ]
    }
   ],
   "source": [
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarization_chain.predict(text=text)\n",
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], \n",
    "template=get_prompt_template(translation_template))\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Votre texte ici\n"
     ]
    }
   ],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "text = \"Your text here\"\n",
    "translated_text = translation_chain.predict(\n",
    "    source_language=source_language, \n",
    "    target_language=target_language, \n",
    "    text=text)\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
