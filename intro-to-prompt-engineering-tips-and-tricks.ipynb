{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UankOoEqfvT5",
        "outputId": "ff8ace58-431a-4901-82fa-53fc3cfaf8ea"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain==0.0.208 openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpnwP9Z6gAtB",
        "outputId": "6b765af3-93c1-4dd8-f3fc-284f3ed59a67"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "OPENAI_API_BASE = os.environ[\"COMPLETIONS_API_BASE\"]\n",
        "SYSTEM_PROMPT_TEMPLATE = \"You are an AI assistant that follows instruction extremely well. Help as much as you can.\"\n",
        "# orca-mini-v3\n",
        "def get_prompt_template(instruction: str):\n",
        "    prompt = f\"### System:\\n{SYSTEM_PROMPT_TEMPLATE}\\n\\n### User:\\n{instruction}\\n\\n### Assistant:\\n\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuflOySdgCsv",
        "outputId": "2203d32c-d46d-4f93-ec5e-7094a1d7ce9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theme: interstellar travel\n",
            "Year: 3030\n",
            "AI-generated song title:  How about \"Cosmic Odyssey: A Journey Through The Stars\" as a potential song title for your interstellar travel theme in the year 3030?\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(openai_api_base=OPENAI_API_BASE, temperature=0)\n",
        "\n",
        "template = \"\"\"\n",
        "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
        "What's a cool song title for a song about {theme} in the year {year}?\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"theme\", \"year\"],\n",
        "    template=get_prompt_template(template),\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the prompt\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Input data for the prompt\n",
        "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"}\n",
        "\n",
        "# Run the LLMChain to get the AI-generated song title\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(\"Theme: interstellar travel\")\n",
        "print(\"Year: 3030\")\n",
        "print(\"AI-generated song title:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_R56C3dgK49",
        "outputId": "188de295-0d9d-496d-b4d9-f1b2c58ce684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Color: purple\n",
            "Emotion:  creativity\n"
          ]
        }
      ],
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
        "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
        "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
        "]\n",
        "\n",
        "example_formatter_template = \"\"\"\n",
        "Color: {color}\n",
        "Emotion: {emotion}\\n\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"color\", \"emotion\"],\n",
        "    template=get_prompt_template(example_formatter_template),\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Here are some examples of colors and the emotions associated with them:\\n\\n\",\n",
        "    suffix=\"\\n\\nNow, given a new color, identify the emotion associated with it:\\n\\nColor: {input}\\nEmotion:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\",\n",
        ")\n",
        "\n",
        "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
        "\n",
        "# Create the LLMChain for the prompt\n",
        "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt, input_variables=[]))\n",
        "\n",
        "# Run the LLMChain to get the AI-generated emotion associated with the input color\n",
        "response = chain.run({})\n",
        "\n",
        "print(\"Color: purple\")\n",
        "print(\"Emotion:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UNsOOLC2gkfJ",
        "outputId": "169a1488-762a-4ead-9e49-b5f1b087d9d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\\n\\n### User:\\nTell me something about dogs.\\n\\n### Assistant:\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = \"Tell me something about {topic}.\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=get_prompt_template(template),\n",
        ")\n",
        "prompt.format(topic=\"dogs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5ZqopkbhDsi",
        "outputId": "a842da0e-3a79-444f-bd3a-1bc754cd5eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scientist: The famous scientist who developed the theory of general relativity is Albert Einstein.\n",
            "Fact:  Albert Einstein's theory of general relativity is a theoretical framework in physics that describes the laws of gravity and its relationship with space and time. It postulates that gravitational attraction is due to the curvature of spacetime caused by the presence of mass and energy, and it explains phenomena such as the gravitational redshift, gravitational lensing, and the precession of Mercury's orbit around the Sun. The theory has significant implications for cosmology, astrophysics, and our understanding of the universe at large.\n"
          ]
        }
      ],
      "source": [
        "# Prompt 1\n",
        "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=get_prompt_template(template_question), input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Provide a brief description of {scientist}'s theory of general relativity.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=get_prompt_template(template_fact))\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Extract the scientist's name from the response\n",
        "scientist = response_question.strip()\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"scientist\": scientist}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(\"Scientist:\", scientist)\n",
        "print(\"Fact:\", response_fact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOByBkJVhaBL",
        "outputId": "34ffbf6f-70a8-4f8f-d594-06181f36f8ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scientist: Albert Einstein\n",
            "Fact:  Albert Einstein was a vegetarian and an advocate for animal rights. He was also a pacifist and a socialist, and he was a strong supporter of the civil rights movement. He was also a passionate violinist and a lover of sailing.\n"
          ]
        }
      ],
      "source": [
        "# Prompt 1\n",
        "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=get_prompt_template(template_question), input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Tell me something interesting about {scientist}.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=get_prompt_template(template_fact))\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Extract the scientist's name from the response\n",
        "scientist = response_question.strip()\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"scientist\": scientist}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(\"Scientist:\", scientist)\n",
        "print(\"Fact:\", response_fact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKzhyWSEiG9H",
        "outputId": "25e50f68-1cec-444f-ec02-a506f73e6b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Genres: jazz pop rock\n",
            "Fact:  Jazz, pop, and rock are all genres of music that have significantly influenced the world of music. Jazz is known for its improvisation, complex harmonies, and rich melodic structures. Pop music, on the other hand, often reflects contemporary trends and appeals to a wide audience with its catchy tunes and relatable lyrics. Rock music originated from blues and folk influences and is characterized by its powerful rhythms, distorted guitar riffs, and raw energy. These genres have evolved over time, incorporating elements of each other and creating new sub-genres that continue to captivate audiences worldwide.\n"
          ]
        }
      ],
      "source": [
        "# Prompt 1\n",
        "template_question = \"\"\"What are some musical genres?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=get_prompt_template(template_question), input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Tell me something about {genre1}, {genre2}, and {genre3} without giving any specific details.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"genre1\", \"genre2\", \"genre3\"], template=get_prompt_template(template_fact))\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Assign three hardcoded genres\n",
        "genre1, genre2, genre3 = \"jazz\", \"pop\", \"rock\"\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"genre1\": genre1, \"genre2\": genre2, \"genre3\": genre3}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(\"Genres:\", genre1, genre2, genre3)\n",
        "print(\"Fact:\", response_fact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-MLn4suilQy",
        "outputId": "6b7f4c3e-981a-4744-b14c-59fcd13e82b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Query: What are some tips for improving communication skills?\n",
            "AI Response: 1. Practice active listening by showing empathy and asking open-ended questions.\n",
            "2. Be mindful of your body language, tone, and choice of words to convey your message effectively.\n",
            "3. Seek feedback from others to identify areas for improvement.\n",
            "4. Engage in regular conversations with people from diverse backgrounds to broaden your perspective.\n"
          ]
        }
      ],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the secret to happiness?\",\n",
        "        \"answer\": \"Finding balance in life and learning to enjoy the small moments.\"\n",
        "    }, {\n",
        "        \"query\": \"How can I become more productive?\",\n",
        "        \"answer\": \"Try prioritizing tasks, setting goals, and maintaining a healthy work-life balance.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=get_prompt_template(example_template)\n",
        ")\n",
        "\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "life coach. The assistant provides insightful and practical advice to the users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the few-shot prompt template\n",
        "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
        "\n",
        "# Define the user query\n",
        "user_query = \"What are some tips for improving communication skills?\"\n",
        "\n",
        "# Run the LLMChain for the user query\n",
        "response = chain.run({\"query\": user_query})\n",
        "\n",
        "print(\"User Query:\", user_query)\n",
        "print(\"AI Response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXPDhFq4i5Ep"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
